# CLI 어댑터 문제 해결을 위한 고민

**작성일**: 2025-11-17
**목적**: 자기비판 리뷰에서 식별된 근본적 문제 해결 전략

---

## 🎯 해결해야 할 핵심 질문들

### 1. 검증의 범위와 깊이

**고민할 문제:**
```
"검증됨"이 무엇을 의미해야 하는가?
```

**세부 질문:**
- 패키지 존재 확인만으로 충분한가?
- 실제 설치 테스트를 해야 하는가? (시간/비용)
- 모든 명령어를 테스트해야 하는가?
- 테스트 환경을 어떻게 구성할 것인가?
- CI/CD에서 자동화할 수 있는가?

**현실적 제약:**
- Atlassian Cloud 구독 비용
- Qwen OAuth 일일 제한 (2,000회)
- 각 CLI의 시스템 요구사항 차이
- 테스트 환경 구축 비용

**선택지:**
1. **최소 검증** (현재): 패키지 존재만 확인
2. **중간 검증**: 설치 + 기본 명령어 테스트 (--version, --help)
3. **완전 검증**: 실제 작업 수행 테스트 (코드 생성, 리뷰 등)

---

### 2. 대화형 인터페이스 자동화 문제

**고민할 문제:**
```
대화형 CLI와 Claude Code의 자동화된 협업이 가능한가?
```

**세부 질문:**
- 대화형 프로그램에 프로그래밍적으로 입력을 전달할 수 있는가?
  - stdin 파이프: `echo "prompt" | qwen`
  - expect/pexpect 스타일 자동화
  - pseudo-terminal 제어
- Claude의 Bash 도구가 대화형 세션을 제어할 수 있는가?
- 사용자 개입 없이 루프를 완료할 수 있는가?

**현실 검증 필요:**
```bash
# 이것이 작동하는가?
echo "Write a hello world function in Python" | qwen

# 또는 이것이 필요한가?
expect -c '
  spawn qwen
  expect ">"
  send "Write a hello world\r"
  expect ">"
  send "/exit\r"
'
```

**선택지:**
1. **자동화 포기**: 대화형 CLI는 dual-ai-loop에서 제외
2. **수동 워크플로우**: 사용자가 복사/붙여넣기
3. **자동화 시도**: expect/pexpect 또는 stdin 파이프 테스트
4. **API 사용**: CLI 대신 직접 API 호출 검토

---

### 3. 아키텍처적 재설계 필요성

**고민할 문제:**
```
모든 CLI를 동일하게 취급하는 것이 올바른가?
```

**현재 문제:**
- codex/aider: 명령줄 기반, 자동화 가능
- qwen/rovo-dev: 대화형 기반, 자동화 제한적
- 동일한 "CLI 어댑터" 패턴으로 묶었지만 실제 동작이 다름

**재설계 옵션:**

**Option A: CLI 타입별 분리**
```
skills/
├── cli-adapters/
│   ├── command-line/      # 명령줄 기반 (자동화 가능)
│   │   ├── codex/
│   │   └── aider/
│   └── interactive/       # 대화형 (수동 필요)
│       ├── qwen/
│       └── rovo-dev/
```

**Option B: 통합 레벨 정의**
```json
{
  "integration_level": "full",      // 완전 자동화
  "integration_level": "partial",   // 일부 자동화
  "integration_level": "manual"     // 수동 가이드만
}
```

**Option C: dual-ai-loop 분리**
```
/dual-ai-loop-automated   # codex, aider 전용
/dual-ai-loop-guided      # qwen, rovo-dev 가이드 제공
```

---

### 4. 사용자 기대치 관리

**고민할 문제:**
```
사용자에게 무엇을 약속하고, 무엇을 약속하지 말아야 하는가?
```

**현재 문제:**
- "Dual-AI Loop"라는 이름이 자동화를 암시
- 실제로는 수동 개입이 많이 필요
- 문서가 기대치를 과장할 수 있음

**명확히 해야 할 것:**
1. **약속할 수 있는 것**:
   - CLI 설치 가이드 제공
   - 명령어 패턴 참조
   - 워크플로우 구조 제안

2. **약속하지 말아야 할 것**:
   - "완전 자동화된 루프"
   - "클릭 한 번으로 실행"
   - "모든 CLI가 동일하게 작동"

3. **솔직히 말해야 할 것**:
   - "대화형 CLI는 수동 개입 필요"
   - "실제 테스트는 수행되지 않았음"
   - "자동화 가능성은 검증 안됨"

---

### 5. 검증 전략 결정

**고민할 문제:**
```
어디까지 검증하고, 어디부터는 사용자에게 맡길 것인가?
```

**스펙트럼:**
```
[패키지 존재] → [설치 가능] → [기본 실행] → [기능 테스트] → [통합 테스트]
    현재 위치              권장 위치              이상적 위치
```

**현실적 검증 계획:**

| 수준 | 내용 | 비용 | 가치 |
|------|------|------|------|
| L1 | 패키지 존재 확인 | 낮음 | 기본 |
| L2 | 설치 성공 확인 | 중간 | 높음 |
| L3 | --help/--version 작동 | 중간 | 높음 |
| L4 | 간단한 작업 수행 | 높음 | 매우 높음 |
| L5 | dual-ai-loop 통합 테스트 | 매우 높음 | 최고 |

**권장**: L3까지는 검증하고, L4-L5는 "미검증" 표시

---

### 6. 유지보수 가능성

**고민할 문제:**
```
CLI가 업데이트될 때 어떻게 대응할 것인가?
```

**시나리오:**
- qwen-code 0.2.1 → 0.3.0으로 업데이트
- rovo-dev 베타 → 정식 버전 출시
- codex가 새로운 명령어 옵션 추가

**필요한 시스템:**
1. **버전 모니터링**: 정기적으로 새 버전 체크
2. **변경 감지**: CHANGELOG 파싱 또는 diff
3. **자동 업데이트**: VERSION.json 갱신
4. **문서 동기화**: SKILL.md 업데이트

**현실적 문제:**
- 수동 체크는 지속 불가능
- 자동화 시스템 구축 비용
- 잘못된 자동 업데이트 위험

---

## 🔍 근본적 질문

### "왜 이것을 하는가?"

현재 상태를 돌아보면:
1. 대화형 CLI는 자동화가 어려움
2. 실제 테스트 없이는 확신할 수 없음
3. 유지보수 비용이 높음

**근본적 의문:**
```
dual-ai-loop가 정말 필요한가?
아니면 단순히 "다른 AI CLI 도구 가이드"가 더 정직한가?
```

**대안적 접근:**
- CLI 사용 가이드 제공 (자동화 주장 없이)
- API 직접 통합 검토 (CLI 대신)
- Claude 단독 워크플로우 개선 (외부 AI 없이)

---

## 📋 결정이 필요한 사항

### 즉시 결정 필요

1. **검증 깊이**: L1 (현재) vs L3 (권장) vs L5 (이상적)
2. **대화형 CLI 처리**: 제외 vs 수동 가이드 vs 자동화 시도
3. **이름 변경**: "Dual-AI Loop" vs "AI CLI 가이드"

### 중기 결정 필요

4. **아키텍처 재설계**: 현재 유지 vs 타입별 분리
5. **유지보수 전략**: 수동 vs 반자동 vs 자동
6. **비용 감수**: 테스트 환경 구축 여부

### 장기 결정 필요

7. **프로젝트 방향**: CLI 통합 vs API 직접 호출
8. **확장성**: 새로운 CLI 추가 기준
9. **품질 기준**: 어느 수준에서 "완료"로 볼 것인가

---

## 🛠 실행 가능한 다음 단계

### Option 1: 현실적 최소화

"할 수 있는 것만 약속하자"
- dual-ai-loop를 "AI CLI 사용 가이드"로 리브랜딩
- 대화형 CLI는 명시적으로 수동 워크플로우로 분류
- 자동화 주장 제거

**장점**: 정직함, 유지보수 부담 감소
**단점**: 기대치 하향, 기능 축소

### Option 2: 검증 강화

"실제로 테스트하고 자신감을 갖자"
- qwen, codex 실제 설치 테스트
- 기본 명령어 실행 확인
- stdin 파이프 가능 여부 테스트

**장점**: 확실한 정보, 신뢰성 향상
**단점**: 시간/비용 투자 필요

### Option 3: 범위 축소

"검증된 것만 지원하자"
- codex와 aider만 공식 지원
- qwen, rovo-dev는 "실험적" 또는 제거
- copilot은 검증 후 결정

**장점**: 품질 집중, 명확한 경계
**단점**: 기능 제한, 사용자 선택권 감소

### Option 4: 아키텍처 재설계

"근본적으로 다시 생각하자"
- CLI 타입별 다른 접근법 적용
- 자동화 가능한 것과 수동 필요한 것 분리
- 각각에 맞는 워크플로우 설계

**장점**: 현실적, 확장성
**단점**: 대규모 재작업 필요

---

## 🎓 결론: 어떤 고민이 필요한가?

### 핵심 3가지 질문

1. **"검증"이란 무엇인가?**
   - 패키지 존재만? 설치 성공? 기능 테스트?
   - 어디까지 하고, 어디서 멈출 것인가?

2. **자동화 vs 가이드**
   - 대화형 CLI와의 자동화가 현실적인가?
   - 아니라면, 수동 가이드로 전환할 것인가?

3. **정직함 vs 야망**
   - 할 수 있는 것만 약속할 것인가?
   - 더 많은 것을 시도하고 실패할 위험을 감수할 것인가?

### 권장 접근법

**단기**: Option 1 + 부분적 Option 2
- 현재 상태를 정직하게 문서화
- 가능한 범위에서 실제 테스트 수행 (npm install 정도)
- 자동화 주장은 검증될 때까지 보류

**중기**: Option 3 또는 Option 4
- 실제 사용 데이터를 바탕으로 결정
- 가장 많이 사용되는 CLI에 집중
- 사용되지 않는 것은 제거 또는 축소

**장기**: 근본적 재평가
- dual-ai-loop가 정말 가치 있는지 평가
- API 직접 통합이 더 나은 선택인지 검토
- CLI 도구 생태계의 변화 관찰

---

## 최종 질문

**사용자에게 물어볼 것:**

1. 어느 수준의 검증이 필요한가요?
2. 대화형 CLI를 어떻게 처리하길 원하나요?
3. "Dual-AI Loop"의 목표가 무엇인가요?
   - 완전 자동화된 협업?
   - 수동 워크플로우 가이드?
   - CLI 도구 사용법 참조?

이 답변에 따라 아키텍처와 접근법이 달라집니다.
